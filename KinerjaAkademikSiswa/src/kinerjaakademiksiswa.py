# -*- coding: utf-8 -*-
"""KinerjaAkademikSiswa.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JvyP2b_Itt_812Ex98NGNv81hbEr-q3u
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
from sklearn.preprocessing import LabelEncoder

from google.colab import files
upload = files.upload()

df_raw = pd.read_csv('student-mat.csv', sep=';')

print("Data Awal:", df_raw.shape)
display(df_raw.head())

df_raw.describe()

# Distribution of G3
plt.figure(figsize=(8,4))
sns.histplot(df_raw['G3'], bins=21, kde=True)
plt.title("Distribution of G3 (Final Grade)")
plt.xlabel("G3")
plt.show()

# 3) Missing values
print("Missing values:\n", df_raw.isnull().sum())

# 4) Create 'pass' column (optional classification)
df_raw['pass'] = (df_raw['G3'] >= 10).astype(int)
print("Pass value counts:\n", df_raw['pass'].value_counts())
# 2️⃣ CEK & TANGANI MISSING VALUE
print("\nJumlah Missing Value per Kolom:")
print(df_raw.isnull().sum())

df_missing = df_raw.copy()
for col in df_missing.columns:
    if df_missing[col].dtype == 'object':
        df_missing[col].fillna(df_missing[col].mode()[0], inplace=True)
    else:
        df_missing[col].fillna(df_missing[col].median(), inplace=True)

print("\nAfter handling missing values:")
print(df_missing.isnull().sum().sum(), "remaining missing values.")

# 3️⃣ ENCODING KATEGORIKAL
df_encoded = df_missing.copy()
label = LabelEncoder()

for col in df_encoded.columns:
    if df_encoded[col].dtype == 'object':
        df_encoded[col] = label.fit_transform(df_encoded[col])

print("\nExample of encoding results:")
display(df_encoded.head())

# Convert G1, G2, and G3 to numeric, coercing errors
df_encoded['G1'] = pd.to_numeric(df_encoded['G1'], errors='coerce')
df_encoded['G2'] = pd.to_numeric(df_encoded['G2'], errors='coerce')
df_encoded['G3'] = pd.to_numeric(df_encoded['G3'], errors='coerce')


# 5) Correlations
plt.figure(figsize=(14,10))
sns.heatmap(df_encoded.corr(), cmap='coolwarm', center=0, annot=False)
plt.title("Correlation matrix")
plt.show()

print("Top correlations with G3:")
print(df_encoded.corr()['G3'].sort_values(ascending=False).head(15))

# 6) Preprocessing
# a) Categorical -> one-hot
cat_cols = df_raw.select_dtypes(include=['object']).columns.tolist()
print("Categorical cols:", cat_cols)
df_enc = pd.get_dummies(df_raw, drop_first=True)

# b) Outlier handling: log1p for 'absences' if skewed
df_outlier = df_encoded.copy()

for col in df_outlier.select_dtypes(include=np.number).columns:
    Q1 = df_outlier[col].quantile(0.25)
    Q3 = df_outlier[col].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    df_outlier = df_outlier[(df_outlier[col] >= lower) & (df_outlier[col] <= upper)]

print("\nData setelah outlier handling:", df_outlier.shape)

# c) Scaling numeric features (excluding target G3 and pass)
num_cols = ['age','Medu','Fedu','traveltime','studytime','failures',
            'famrel','freetime','goout','Dalc','Walc','health','absences','G1','G2']
# ensure present
num_cols = [c for c in num_cols if c in df_enc.columns]
scaler = StandardScaler()
df_enc[num_cols] = scaler.fit_transform(df_enc[num_cols])

# 7) Prepare data for modeling
X = df_enc.drop(['G3','pass'], axis=1)   # for regression
y_reg = df_enc['G3']
y_clf = df_enc['pass']  # classification

# train-test split
X_tr, X_te, ytr_reg, yte_reg = train_test_split(X, y_reg, test_size=0.2, random_state=42)
_, _, ytr_clf, yte_clf = train_test_split(X, y_clf, test_size=0.2, random_state=42)

print("Train/Test shapes:", X_tr.shape, X_te.shape)

# 8) Regression models
# Linear Regression
lr = LinearRegression()
lr.fit(X_tr, ytr_reg)
y_pred_lr = lr.predict(X_te)
print("LinearRegression - MAE:", mean_absolute_error(yte_reg, y_pred_lr))
print("LinearRegression - RMSE:", np.sqrt(mean_squared_error(yte_reg, y_pred_lr)))
print("LinearRegression - R2:", r2_score(yte_reg, y_pred_lr))

# Random Forest Regressor (basic)
rfr = RandomForestRegressor(random_state=42, n_estimators=100)
rfr.fit(X_tr, ytr_reg)
y_pred_rfr = rfr.predict(X_te)
print("RandomForestRegressor - MAE:", mean_absolute_error(yte_reg, y_pred_rfr))
print("RandomForestRegressor - RMSE:", np.sqrt(mean_squared_error(yte_reg, y_pred_rfr)))
print("RandomForestRegressor - R2:", r2_score(yte_reg, y_pred_rfr))

# Feature importance (regressor)
importances = pd.Series(rfr.feature_importances_, index=X.columns).sort_values(ascending=False)
print("Top 20 feature importances (RFR):")
print(importances.head(20))

# 9) Classification models (predict 'pass')
# Logistic Regression
log = LogisticRegression(max_iter=1000)
log.fit(X_tr, ytr_clf)
y_pred_log = log.predict(X_te)
print("LogisticRegression - Acc:", accuracy_score(yte_clf, y_pred_log))
print(classification_report(yte_clf, y_pred_log))

# Random Forest Classifier
rfc = RandomForestClassifier(random_state=42, n_estimators=200)
rfc.fit(X_tr, ytr_clf)
y_pred_rfc = rfc.predict(X_te)
print("RandomForestClassifier - Acc:", accuracy_score(yte_clf, y_pred_rfc))
print(classification_report(yte_clf, y_pred_rfc))
cm = confusion_matrix(yte_clf, y_pred_rfc)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - RFC")
plt.show()

# Feature importance (classifier)
imp_clf = pd.Series(rfc.feature_importances_, index=X.columns).sort_values(ascending=False)
print("Top 20 feature importances (RFC):")
print(imp_clf.head(20))

# 10) Save models or results (optional)
from joblib import dump
dump(rfr, 'rfr_student.joblib')
dump(rfc, 'rfc_student.joblib')

# End of notebook